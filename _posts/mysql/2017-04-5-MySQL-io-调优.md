---
layout: article
title: "什么导致MySQL数据库服务器磁盘I/O高?"
modified:
categories: mysql
toc: true
ads: true
image:
    teaser: /teaser/question.jpg
---

> InnoDB log机制及优化

---   
### InnoDB log机制及优化
**一.当更新数据时，innodb内部的操作流程大致如下：**
1. 数据读入innodb buffer pool， 并对相关记录加独占锁；  
2. 将undo信息写入undo表空间的回滚段中；  
3. 更改缓存页码=中的数据，并将更新记录写入 redo buffer 中；  
4. 提交时，将innodb_flush_log_at_trx_commit 的资源，用不通的方式将redo buffer中的更新记录刷新到innodb redo log file中，然后释放独占锁；  
5. 最后，后台IO线程根据需要择机将缓存中更新过得数据刷新到磁盘文件中。

可以通过，`show engine innodb status\G`查看当前日志的写入情况：
{% highlight mysql %}
{% raw %}
---
LOG  当前日志的写入情况
---
Log sequence number 539020439942   上次数据页的修改还没有刷新到日志文件的LSN号
Log flushed up to   539020439942   上次成功操作，已经刷新到日志文件中的LSN号
Pages flushed up to 539020432879	
Last checkpoint at  539020430783	上次检查点成功完成时的LSN号以为着回复的起点
0 pending log writes, 0 pending chkp writes
20916879 log i/o's done, 1.75 log i/o's/second

LSN：日志序列号，对应日志文件的偏移量，其生成公式：
  新的LSN = 旧的LSN + 日志写入大小
{% endraw %}
{% endhighlight %}
  
**二.innodb status解析：**
{% highlight mysql %}
{% raw %}
mysql>show engine  innodb status;
  Type: InnoDB
  Name: 
Status: 
=====================================
2017-04-05 18:41:21 7eff3ffff700 INNODB MONITOR OUTPUT
=====================================
Per secondaverages calculated from the last 0 seconds

----------
SEMAPHORES
----------
主要显示系统的当前的信号等待信息及各种等待信号的统计信息，这部分对调整innodb_thread_concurrency参数有非常大的帮助。当等
待信号量非常大的时候，可能就需要禁用并发线程检测设置innodb_thread_concurrency=0;

------------
TRANSACTIONS
------------
主要展示系统的锁等待信息和当前活动事务信息，通过这部分，可以追踪到死锁的详细信息。

--------
FILE I/O
--------
文件I/O相关信息，主要是IO等待信息。

-------------------------------------
INSERTBUFFER AND ADAPTIVE HASH INDEX
-------------------------------------
显示插入缓存当前状态信息及自适应hash index的状态

---
LOG
---
Innodb 事务日志相关信息，包括当前的日志序列号（Log sequence number），已经刷新同步到那个序列号，最近的check point到那
个序列号了。除此之外，还显示了系统从启动到现在已经做了多少次check point，多少次日志刷新。

----------------------
BUFFER POOLAND MEMORY
----------------------
主要显示innodb buffer pool相关的各种统计信息，以及其他一些内存使用情况。

--------------
ROW OPERATIONS
--------------
主要显示的是与客户端的请求query和query所影响的记录统计信息
{% endraw %}
{% endhighlight %}
---
### 故障解决实例
##### 实例一:JDB2导致磁盘io使用率高,导致mysql延迟过高
{% highlight mysql %}
{% raw %}
**1.查看磁盘io状况**
[root@putuo-mysql ~]# iostat -dxm 1
Linux 2.6.32-573.el6.x86_64 (putuo-mysql) 	07/05/2017 	_x86_64_	(32 CPU)

Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await  svctm  %util
sda               0.00    73.86    0.00    8.40     0.00     0.32    78.34     0.09   10.08   4.15   3.49
dm-0              0.00     0.00    0.00    8.78     0.00     0.03     8.00     0.02    2.53   0.32   0.28
dm-1              0.00     0.00    0.00    0.00     0.00     0.00     8.00     0.00    1.44   1.31   0.00
dm-2              0.00     0.00    0.00   73.49     0.00     0.29     8.00     0.03    0.42   0.45   3.27
{% endraw %}
{% endhighlight %}

{% highlight mysql %}
{% raw %}
[root@putuo-mysql ~]# iotop 

Total DISK READ: 0.00 B/s | Total DISK WRITE: 823.20 K/s
  TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN     IO>    COMMAND                        
 2882 be/3 root        0.00 B/s    0.00 B/s  0.00 %  77.30 % [jbd2/dm-2-8]
24617 be/4 mysql       0.00 B/s    0.00 B/s  0.00 %  78.96 % mysqld --basedir~.sock --port=9036
24644 be/4 mysql       0.00 B/s    3.87 K/s  0.00 %  45.00 % mysqld --basedir~.sock --port=9036
64753 be/4 root        0.00 B/s    3.87 K/s  0.00 %  0.00 % sh /root/lw/script/db_back.s
可以看到io相当繁忙，iowait也很高，并且[jbd2/dm-2-8]占用了io很大的资源
{% endraw %}
{% endhighlight %}

**2.优化sql**
iowait随机io那么高,那么我们优化sql，结果发现没效果


**3.其它方法**
iotop [jbd2/dm-2-8]过高，搜索了一下，是ext4文件系统的一个bug，修复过后 slave延迟果然开始降低了
[**参考链接**](http://www.bubuko.com/infodetail-971804.html)

**4.后续**
修复上诉bug后情况仍不见好转，用dstat命令看下io前几名的进程
{% highlight mysql %}
{% raw %}
[root@putuo-mysql ~]# dstat  -lrt --top-io
---load-avg--- --io/total- ----system---- ----most-expensive----
read  writ | 1m   5m  15m |  date/time   |  block i/o process      
1.00   134 |2.10 2.19 2.21|26-07 15:42:23|mysqld     4096B  1706k
1.00   142 |2.10 2.19 2.21|26-07 15:42:24|mysqld     4096B  1280k
1.00   118 |2.17 2.20 2.21|26-07 15:42:25|mysqld     4096B  1786k
   0   133 |2.17 2.20 2.21|26-07 15:42:26|mysqld        0   1792k
   0   119 |2.17 2.20 2.21|26-07 15:42:27|mysqld        0   1792k
1.00   116 |2.17 2.20 2.21|26-07 15:42:28|mysqld     4096B  1724k

mysql> show global status where variable_name in('Innodb_log_waits','Innodb_buffer_pool_wait_free');
+------------------------------+-------+
| Variable_name                | Value |
+------------------------------+-------+
| Innodb_buffer_pool_wait_free | 0     |
| Innodb_log_waits             | 4776  |   ## log buffer过小导致写日志等待数。单位是次。
+------------------------------+-------+
2 rows in set (0.00 sec) 
{% endraw %}
{% endhighlight %}
**5.解决**
加大log_buffer_pool与innodb_log_file_size的值，重启数据库，发现iowait降低.

---
##### 实例二:innodb I/O调优之日志文件与日志缓存（一）
{% highlight mysql %}
{% raw %}
mysql> show global status like 'Innodb_os_log_written';select sleep(60);show global status like 'Innodb_os_log_written'; 
ERROR 2006 (HY000): MySQL server has gone away
No connection. Trying to reconnect...
Connection id:    249025
Current database: *** NONE ***

+-----------------------+--------------+
| Variable_name         | Value        |
+-----------------------+--------------+
| Innodb_os_log_written | 326041253376 |
+-----------------------+--------------+
1 row in set (0.00 sec)

+-----------+
| sleep(60) |
+-----------+
|         0 |
+-----------+
1 row in set (59.99 sec)

+-----------------------+--------------+
| Variable_name         | Value        |
+-----------------------+--------------+
| Innodb_os_log_written | 326041380352 |
+-----------------------+--------------+
1 row in set (0.00 sec)

mysql>  select (326041380352-326041253376)*60 /1024/1204 as MB_PER_HOUR;
ERROR 2006 (HY000): MySQL server has gone away
No connection. Trying to reconnect...
Connection id:    249091
Current database: *** NONE ***

+-------------+
| MB_PER_HOUR |
+-------------+
|  6.17940199 |
+-------------+
1 row in set (0.00 sec)

MySQL默认使用两个日志文件，设置innodb_log_file_size=500M
 
使用以下方式调整日志文件大小：
You usually don’t need to change the default number of logs, just the size of each log file. To change the 
log file size, shut
down MySQL cleanly, move the old logs away, reconfigure, and restart. Be sure MySQL
shuts down cleanly, or the log files will actually have entries that need to be applied to
the data files! Watch the MySQL error log when you restart the server. After you’ve
restarted successfully, you can delete the old log files.

设置日志缓存大小
在innodb改变数据的时候，它会把这次改动的记录写到日志缓冲里面。日志缓冲被保存在内存中。缓冲写满、事务提交或每一秒钟，不管哪
种情况先发生，Innodb都会把缓冲区写到磁盘上的日志文件中。如果有大型事务，可以增加innodb_log_buffer_size的大小（默认为
8M）来减少I/O动作。不需要把日志缓冲区设置的很大，推荐的设置是1M到8M。除非写入大量的巨型blob记录或大事务，否则这个大小就足
够了。


mysql> show global status like 'Innodb_log_waits'; 
+------------------+-------+
| Variable_name    | Value |
+------------------+-------+
| Innodb_log_waits | 4776  |
+------------------+-------+
1 row in set (0.00 sec)
如果Innodb_log_waits状态变量（等待日志缓冲刷出的次数）的值比较高，而且继续增长，可以增大log buffer或者降低事务大小。
 
innodb_log_file_size与innodb_log_buffer_size的设置需要重启服务器。

参考:《高性能MySQL (第二版)》
{% endraw %}
{% endhighlight %}
---
##### 实例三：innodb I/O调优之日志文件与日志缓存（二）
{% highlight mysql %}
{% raw %}
mysql> show  variables like '%innodb_log%';
+-----------------------------+-----------+
| Variable_name               | Value     |
+-----------------------------+-----------+
| innodb_log_buffer_size      | 3145728   |
| innodb_log_compressed_pages | ON        |
| innodb_log_file_size        | 268435456 |
+-----------------------------+-----------+

mysql> show  variables like '%log_bu%';
+------------------------+---------+
| Variable_name          | Value   |
+------------------------+---------+
| innodb_log_buffer_size | 3145728 |
+------------------------+---------+
1 row in set (0.00 sec)

mysql> pager grep -i 'log sequence number';
PAGER set to 'grep -i 'log sequence number''
mysql> show engine innodb status\G select sleep(60);show engine innodb status\G;
ERROR 2006 (HY000): MySQL server has gone away
No connection. Trying to reconnect...
Connection id:    249229
Current database: *** NONE ***

Log sequence number 539018616135
1 row in set (0.00 sec)

1 row in set (59.99 sec)

Log sequence number 539018700412
1 row in set (0.00 sec)

mysql> nopager
PAGER set to stdout
mysql>   select round((539018700412 - 539018616135)/1024 / 1024) as MB;
+------+
| MB   |
+------+
|    0 |
+------+
1 row in set (0.00 sec)

参考:《深入浅出MySQL》
{% endraw %}
{% endhighlight %}
---
##### 实例四：是什么导致MySQL数据库服务器磁盘I/O高？
 
**1、问题**
MySQL服务器最总是报警磁盘I/O非常高：

一般来说，磁盘I/O很高无非是下面几个原因引起：  

1.磁盘子系统设备性能差，或采用ext2/ext3之类文件系统，或采用cfq之类的io scheduler，所以IOPS提上不去；    
2.SQL效率不高，比如没有索引，或者一次性读取大量数据，所以需要更多的I/O；    
3.可用内存太小，内存中能缓存/缓冲的数据不多，所以需要更多的I/O。    

**2、排查**

先看磁盘I/O设备，是由十几块SSD组成的RAID 10阵列，按理说I/O性能应该不至于太差，看iops和%util的数据也确实如此。
{% highlight mysql %}
{% raw %}
[root@putuo-mysql ~]# iostat -dxm 1
Linux 2.6.32-573.el6.x86_64 (putuo-mysql) 	07/05/2017 	_x86_64_	(32 CPU)

Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await  svctm  %util
sda               0.00    73.86    0.00    8.40     0.00     0.32    78.34     0.09   10.08   4.15   3.49
dm-0              0.00     0.00    0.00    8.78     0.00     0.03     8.00     0.02    2.53   0.32   0.28
dm-1              0.00     0.00    0.00    0.00     0.00     0.00     8.00     0.00    1.44   1.31   0.00
dm-2              0.00     0.00    0.00   73.49     0.00     0.29     8.00     0.03    0.42   0.45   3.27


Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await  svctm  %util   //主要后面三个值
sda               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00   0.00   0.00
dm-0              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00   0.00   0.00
dm-1              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00   0.00   0.00
dm-2              0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00   0.00   0.00
{% endraw %}
{% endhighlight %}
再来看下文件系统、io scheduler的因素，发现采用xfs文件系统，而且io scheduler用的是noop，看来也不是这个原因。而且看了下
iostat的数据，发现iops也不算低，说明I/O能力还是可以的。

再来看看当前的processlist，以及slow query log，也没发现当前有特别明显的slow query，所以也不是这个原因了。
{% highlight mysql %}
{% raw %}
mysql> show processlist;
+--------+-----------------+-----------------------+----------------------+---------+------+-----------------------------+------------------+
| Id     | User            | Host                  | db                   | Command | Time | State                       | Info             |
+--------+-----------------+-----------------------+----------------------+---------+------+-----------------------------+------------------+
|      1 | event_scheduler | localhost             | NULL                 | Daemon  | 2674 | Waiting for next activation | NULL             |
|  11477 | health          | xxxx:xxx | heaxxxx_dxox     | Sleep   |  112 |                             | NULL             |
| 193308 | health          | xxxx:xxx | xxxclxud_txxs    | Sleep   |  276 |                             | NULL             |
| 247955 | health          | xxxx:xxx | heaxxxxoud_lxx   | Sleep   |  256 |                             | NULL             |
| 249078 | health          | xxxx:xxx | hexxxud_clxxic   | Sleep   |    5 |                             | NULL             |
| 249221 | health          | xxxx:xxx | hltoud_doxx      | Sleep   | 1721 |                             | NULL             |
{% endraw %}
{% endhighlight %}

现在只剩下内存不足这个因素了，看了下服务器物理内存是128G，用系统命令 free 看了下，发现大部分都在cached，而free的也不多。观察InnoDB相关的配置以及status，看能不能找到端倪。
{% highlight mysql %}
{% raw %}
mysql> show variables like '%innodb_buffer_pool_size%';
+-------------------------+-------------+
| Variable_name           | Value       |
+-------------------------+-------------+
| innodb_buffer_pool_size | 68719476736 |
+-------------------------+-------------+
1 row in set (0.01 sec)

再看一下 innodb status：
mysql> show global status like '%innodb_buffer_pool_pages_free%';
+-------------------------------+---------+
| Variable_name                 | Value   |
+-------------------------------+---------+
| Innodb_buffer_pool_pages_free | 2584840 |
+-------------------------------+---------+
1 row in set (0.00 sec)

mysql> show global status like '%innodb_buffer_pool_pages_free%';
+-------------------------------+---------+
| Variable_name                 | Value   |
+-------------------------------+---------+
| Innodb_buffer_pool_pages_free | 2584839 |
+-------------------------------+---------+
| Innodb_buffer_pool_wait_free  | 0       |
+-------------------------------+---------+
| Innodb_log_waits              | 4776    |
+-------------------------------+---------+
| Innodb_row_lock_current_waits | 0       |
+---------------------------------+-------+
| Innodb_row_lock_waits         | 147     |
+-------------------------------+---------+
1 row in set (0.00 sec)
{% endraw %}
{% endhighlight %}

重点关注下几个wait值，再看下show engine innodb结果：
{% highlight mysql %}
{% raw %}
------------
TRANSACTIONS
------------
Trx id counter 55616954
Purge done for trx's n:o < 55616798 undo n:o < 0 state: running but idle
History list length 1492
LIST OF TRANSACTIONS FOR EACH SESSION:
---TRANSACTION 0, not started
MySQL thread id 249747, OS thread handle 0x7f00aebaf700, query id 568798995 localhost root init
show engine innodb status
{% endraw %}
{% endhighlight %}

关注下unpurge列表大小，看起来还是比较大的（1492）。
更为诡异的是，在已经停掉SLAVE IO & SQL线程后，发现redo log还在一直增长...
第一次看：
{% highlight mysql %}
{% raw %}
---
LOG  查看当前日志的写入情况
---
Log sequence number 539020439942   上次数据页的修改还没有刷新到日志文件的LSN号
Log flushed up to   539020439942   上次成功操作，已经刷新到日志文件中的LSN号
Pages flushed up to 539020432879	
Last checkpoint at  539020430783   上次检查点成功完成时的LSN号以为着回复的起点
0 pending log writes, 0 pending chkp writes
20916879 log i/o's done, 1.75 log i/o's/second


停掉SLAVE线程后过阵子再看：
---
LOG
---
Log sequence number 539020439942
Log flushed up to   539020449788
Pages flushed up to 539020432977
Last checkpoint at  539020430783
0 pending log writes, 0 pending chkp writes
20916879 log i/o's done, 1.75 log i/o's/second
{% endraw %}
{% endhighlight %}

看到这里，主要是因为 innodb buffer pool 太小，导致了下面几个后果：

1.`dirty page` 和 `data page` 之间相互“排挤抢占”，所以会出现  `Innodb_buffer_pool_wait_free` 事件；
2.`redo log` 也没办法及时刷新到磁盘中，所以在SLAVE线程停掉后，能看到LSN还在持续增长；
3.同时我们也看到unpurge的列表也积攒到很大（111万），这导致了ibdata1文件涨到了146G之大，不过这个可能也是因为有某些事务长
时间未提交。

还有，不知道大家注意到没，`Innodb_row_lock_current_waits` 的值竟然是 `18446744073709551615`（想想bigint多大），显然不可
能啊。事实上，这种情况已经碰到过几次了，明明当前没有行锁，这个 status 值却不小，查了一下官方bug库，竟然只报告了一例，bug 
id是#71520。

**3、解决**
既然知道原因，问题解决起来也就快了，我们主要做了下面几个调整：
调大`innodb-buffer-pool-size`，原则上不超过物理内存的70%，所以设置为40G；
调大`innodb-purge-thread`，原来是1，调整成4；
调大`innodb_io_capacity和innodb_io_capacity_max`，值分别为2万和2.5万；
调整完后，重启实例（5.7版本前调整`innodb-buffer-pool-size` 和 `innodb-purge-thread` 需要**重启**才生效）。再经观察，发现IOPS下降的很快，不再告警，同时 `Innodb_buffer_pool_wait_free` 也一直为 0，unpurge列表降到了数千级别。



